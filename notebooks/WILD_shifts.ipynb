{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Precursor Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "root = Path().resolve().parent\n",
    "if str(root) not in sys.path:\n",
    "    sys.path.insert(0, '..')\n",
    "# Standard Libraries\n",
    "import numpy as np\n",
    "import matplotlib.patches as mpatches\n",
    "%matplotlib inline\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import matplotlib.transforms as transforms\n",
    "import string\n",
    "from utils.utils import *\n",
    "from utils.plots import *\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_letters(fig, ax, dx=-35/72., dy=15/72.):\n",
    "    \n",
    "    letterkwargs = dict(weight='bold', va='top', ha='left')\n",
    "\n",
    "    offset = transforms.ScaledTranslation(\n",
    "            dx, dy, fig.dpi_scale_trans)\n",
    "\n",
    "    for idx in range(len(ax)):\n",
    "        ax[idx].text(0, 1, string.ascii_lowercase[idx], transform=ax[idx].transAxes + offset, \n",
    "                    **letterkwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load metrics basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_FOLDER_NAME = '../results'\n",
    "\n",
    "# Load the metrics for all models\n",
    "with open(os.path.join(f'{RESULTS_FOLDER_NAME}', 'realized_metrics_dict.pkl'), 'rb') as f:\n",
    "    realized_metrics_dict = pkl.load(f)\n",
    "with open(os.path.join(f'{RESULTS_FOLDER_NAME}', 'ATC_metrics_dict.pkl'), 'rb') as f:\n",
    "    ATC_metrics_dict = pkl.load(f)\n",
    "with open(os.path.join(f'{RESULTS_FOLDER_NAME}', 'CBPE_metrics_dict.pkl'), 'rb') as f:\n",
    "    CBPE_metrics_dict = pkl.load(f)\n",
    "with open(os.path.join(f'{RESULTS_FOLDER_NAME}', 'CM_ATC_metrics_dict.pkl'), 'rb') as f:\n",
    "    CMATC_metrics_dict = pkl.load(f)\n",
    "with open(os.path.join(f'{RESULTS_FOLDER_NAME}', 'DoC_metrics_dict.pkl'), 'rb') as f:\n",
    "    DoC_metrics_dict = pkl.load(f)\n",
    "with open(os.path.join(f'{RESULTS_FOLDER_NAME}', 'CM_DoC_metrics_dict.pkl'), 'rb') as f:\n",
    "    CMDoC_metrics_dict = pkl.load(f)\n",
    "\n",
    "FIGURE_WIDTH = 4.803 # inches\n",
    "MODEL_NAMES = list(realized_metrics_dict.keys())\n",
    "\n",
    "METRICS = ['bal_accuracy','recall', 'specificity', 'auc', 'accuracy', 'precision', 'f1_score',]\n",
    "SEEDS = [f'seed_{i}' for i in range(1, 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE_dict = {'CBPE': {calib: {distr: {metric: [] for metric in METRICS} for distr in ['id_test', 'ood_test']} for calib in ['uncal', 'TS', 'CWTS',]},\n",
    "            'ATC': {calib: {distr: {metric: [] for metric in METRICS} for distr in ['id_test', 'ood_test']} for calib in ['uncal', 'TS', 'CWTS',]},\n",
    "            'CMATC': {calib: {distr: {metric: [] for metric in METRICS} for distr in ['id_test', 'ood_test']} for calib in ['uncal', 'TS', 'CWTS',]},\n",
    "            'DoC': {calib: {distr: {metric: [] for metric in METRICS} for distr in ['id_test', 'ood_test']} for calib in ['uncal', 'TS', 'CWTS',]},\n",
    "            'CMDoC': {calib: {distr: {metric: [] for metric in METRICS} for distr in ['id_test', 'ood_test']} for calib in ['uncal', 'TS', 'CWTS',]},\n",
    "            }\n",
    "\n",
    "\n",
    "for model_name in MODEL_NAMES:\n",
    "    for seed in SEEDS:\n",
    "        if seed != 'seed_1':  # For now only look at seed 1\n",
    "            continue\n",
    "        for n, metric in enumerate(METRICS):\n",
    "            for calib in ['uncal', 'TS', 'CWTS']:\n",
    "                for m, distr in enumerate(['id_test', 'ood1_test', 'ood2_test']):\n",
    "                    if calib == 'uncal':   \n",
    "                        realized = realized_metrics_dict[model_name][seed][distr][metric]\n",
    "                        ATC = ATC_metrics_dict[model_name][calib][seed][distr][metric]\n",
    "                        CBPE = CBPE_metrics_dict[model_name][calib][seed][distr][metric]\n",
    "                        CMATC = CMATC_metrics_dict[model_name][calib][seed][distr][metric]\n",
    "                        DoC = DoC_metrics_dict[model_name][calib][seed][distr][metric]\n",
    "                        CMDoC = CMDoC_metrics_dict[model_name][calib][seed][distr][metric]\n",
    "                    elif calib == 'CWTS':\n",
    "                        realized = realized_metrics_dict[model_name][seed][distr][metric]\n",
    "                        CBPE = CBPE_metrics_dict[model_name][calib][seed][distr][metric]\n",
    "\n",
    "                    else:\n",
    "                        continue\n",
    "                    # Calculate MAE for each metric\n",
    "                    if 'ood' in distr:\n",
    "                        distr = 'ood_test'\n",
    "    \n",
    "                    MAE_dict['CBPE'][calib][distr][metric].append(np.abs(realized - CBPE))\n",
    "                    MAE_dict['ATC'][calib][distr][metric].append(np.abs(realized - ATC))\n",
    "                    MAE_dict['CMATC'][calib][distr][metric].append(np.abs(realized - CMATC))\n",
    "                    MAE_dict['DoC'][calib][distr][metric].append(np.abs(realized - DoC))\n",
    "                    MAE_dict['CMDoC'][calib][distr][metric].append(np.abs(realized - CMDoC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE_dict.keys()\n",
    "id_mae_metric_dict = {metric: [] for metric in METRICS}\n",
    "ood_mae_metric_dict = {metric: [] for metric in METRICS}\n",
    "for method in ['CBPE', 'ATC', 'CMATC', 'DoC', 'CMDoC',]:\n",
    "    for calib in ['uncal']:\n",
    "        for distr in ['id_test', 'ood_test']:\n",
    "            for metric in ['accuracy']:\n",
    "                mae = (np.mean(MAE_dict[method][calib][distr][metric]))\n",
    "                if distr == 'id_test':\n",
    "                    id_mae_metric_dict[metric].append(mae)\n",
    "                else:\n",
    "                    ood_mae_metric_dict[metric].append(mae)\n",
    "                    \n",
    "                \n",
    "\n",
    "print(f'MAE ID Accuracy = {np.mean(id_mae_metric_dict[\"accuracy\"])} +- {np.std(id_mae_metric_dict[\"accuracy\"])}')\n",
    "print(f'MAE OOD Accuracy = {np.mean(ood_mae_metric_dict[\"accuracy\"])} +- {np.std(ood_mae_metric_dict[\"accuracy\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper Figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['hatch.linewidth'] = 0.5\n",
    "cmap_b = plt.get_cmap('Blues')  # same hue, different lightness\n",
    "cmap_r = plt.get_cmap('Reds')\n",
    "\n",
    "color_mapping = {'test': 'k',\n",
    "                'validation': 'grey',\n",
    "                'CBPE': 'g',\n",
    "                'ATC': cmap_b(0.55),\n",
    "                'CMATC': cmap_b(0.9),\n",
    "                'DoC': cmap_r(0.55),\n",
    "                'CMDoC': cmap_r(0.9),\n",
    "                }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with plt.style.context('../config/plot_style.txt'): # type: ignore\n",
    "    fig, axs = plt.subplots(2, 4, figsize=(FIGURE_WIDTH, 0.5*FIGURE_WIDTH), layout='constrained', sharey=True, sharex=False) #TODO: FOr no 2times figsize for better view\n",
    "\n",
    "    axs = axs.flatten()\n",
    "    add_letters(fig, axs, dx=-1/72, dy=8/72) # Add letters to the subplots\n",
    "    for n, metric in enumerate(METRICS):\n",
    "\n",
    "        if metric == 'bal_accuracy':\n",
    "            axs[n].set_title('bal. accuracy')\n",
    "        elif metric == 'f1_score':\n",
    "            axs[n].set_title('F1-score')\n",
    "        elif metric == 'auc':\n",
    "            axs[n].set_title('AUC')\n",
    "        elif metric == 'precision':\n",
    "            axs[n].set_title('PPV')\n",
    "        else:\n",
    "            axs[n].set_title(metric)\n",
    "        axs[n].set_yticks([0, 0.2, 0.4, ])\n",
    "        for m, calib in enumerate(['uncal']):   \n",
    "            for k, distr in enumerate(['id_test', 'ood_test']):\n",
    "                CBPE_mae = np.mean(MAE_dict['CBPE'][calib][distr][metric])\n",
    "                ATC_mae = np.mean(MAE_dict['ATC'][calib][distr][metric])\n",
    "                CMATC_mae = np.mean(MAE_dict['CMATC'][calib][distr][metric])\n",
    "                DoC_mae = np.mean(MAE_dict['DoC'][calib][distr][metric])\n",
    "                CMDoC_mae = np.mean(MAE_dict['CMDoC'][calib][distr][metric])\n",
    "\n",
    "                CBPE_std = np.std(MAE_dict['CBPE'][calib][distr][metric])\n",
    "                ATC_std = np.std(MAE_dict['ATC'][calib][distr][metric])\n",
    "                CMATC_std = np.std(MAE_dict['CMATC'][calib][distr][metric])\n",
    "                DoC_std = np.std(MAE_dict['DoC'][calib][distr][metric])\n",
    "                CMDoC_std = np.std(MAE_dict['CMDoC'][calib][distr][metric])\n",
    "\n",
    "                                \n",
    "                alpha = 1 if distr == 'ood_test' else 1\n",
    "                hatch = '//////' if distr == 'ood_test' else ''\n",
    "                axs[n].bar(k*0.2, CBPE_mae, width=0.2, linewidth=0.5,label='CBPE', edgecolor='black', color=color_mapping['CBPE'], alpha=alpha, hatch=hatch,)# yerr=CBPE_std, capsize=1)\n",
    "                axs[n].bar(k*0.2 + 0.5, ATC_mae, width=0.2, label='ATC', color=color_mapping['ATC'], edgecolor='black', \n",
    "                            linewidth=0.5, alpha=alpha, hatch=hatch,)# yerr=ATC_std, capsize=1)\n",
    "                axs[n].bar(k*0.2 + 1., CMATC_mae, width=0.2,linewidth=0.5, label='CMATC', color=color_mapping['CMATC'], edgecolor='black', alpha=alpha, hatch=hatch,)# yerr=CMATC_std, capsize=1)\n",
    "                axs[n].bar(k*0.2 + 1.5, DoC_mae, width=0.2,linewidth=0.5, label='DoC', color=color_mapping['DoC'], edgecolor='black', alpha=alpha, hatch=hatch,)# yerr=DoC_std, capsize=1)\n",
    "                axs[n].bar(k*0.2 + 2., CMDoC_mae, width=0.2,linewidth=0.5, label='CMDoC', color=color_mapping['CMDoC'], edgecolor='black', alpha=alpha, hatch=hatch,)# yerr=CMDoC_std, capsize=1)\n",
    "\n",
    "\n",
    "\n",
    "                if n >=4:\n",
    "                    axs[n].set_xticks([0.1, 0.6, 1.1, 1.6, 2.1])\n",
    "                    axs[n].set_xticklabels(['CBPE', 'ATC', 'CM-\\nATC', 'DoC', 'CM-\\nDoC',], rotation=0, ha='center',)\n",
    "                else:\n",
    "                    axs[n].set_xticks([0.1, 0.6, 1.1, 1.6, 2.1])\n",
    "                    axs[n].set_xticklabels([])\n",
    "\n",
    "                    \n",
    "\n",
    "    ax = axs[-1]\n",
    "    for l, calib in enumerate(['', 'TS_', 'CWTS_', 'DE_']):\n",
    "        if calib != '':\n",
    "            continue  \n",
    "        ax.set_title('Calibration')\n",
    "        \n",
    "        rbs = np.mean([realized_metrics_dict[model_name]['seed_1']['id_test'][f'{calib}rbs'] for model_name in MODEL_NAMES])\n",
    "        rbs_std = np.std([realized_metrics_dict[model_name]['seed_1']['id_test'][f'{calib}rbs'] for model_name in MODEL_NAMES])\n",
    "        ace = np.mean([realized_metrics_dict[model_name]['seed_1']['id_test'][f'{calib}ACE'] for model_name in MODEL_NAMES])\n",
    "        ace_std = np.std([realized_metrics_dict[model_name]['seed_1']['id_test'][f'{calib}ACE'] for model_name in MODEL_NAMES])\n",
    "        ax.bar(0, rbs, width=0.2, linewidth=0.5,label='RBS', color='grey', edgecolor='black',alpha=1,)\n",
    "        ax.bar(0.5, ace, width=0.2, linewidth=0.5,label='ACE', color='orange', edgecolor='black', alpha=1,)\n",
    "        \n",
    "        rbs = np.mean([[realized_metrics_dict[model_name]['seed_1'][distr][f'{calib}rbs'] for distr in ['ood1_test', 'ood2_test'] ] for model_name in MODEL_NAMES])\n",
    "        rbs_std = np.std([[realized_metrics_dict[model_name]['seed_1'][distr][f'{calib}rbs'] for distr in ['ood1_test', 'ood2_test'] ] for model_name in MODEL_NAMES])\n",
    "        ace = np.mean([[realized_metrics_dict[model_name]['seed_1'][distr][f'{calib}ACE'] for distr in ['ood1_test', 'ood2_test'] ] for model_name in MODEL_NAMES])\n",
    "        ace_std = np.std([[realized_metrics_dict[model_name]['seed_1'][distr][f'{calib}ACE'] for distr in ['ood1_test', 'ood2_test'] ] for model_name in MODEL_NAMES])\n",
    "        ax.bar(0.2, rbs, width=0.2,linewidth=0.5, color='grey', edgecolor='black', hatch='///', )\n",
    "        ax.bar(0.7, ace, width=0.2,linewidth=0.5, color='orange', edgecolor='black', hatch='///', )\n",
    "        \n",
    "        ax.set_xticks([0.1, 0.6])\n",
    "        ax.set_xticklabels(['RBS', 'ACE'], rotation=0, ha='center')\n",
    "        \n",
    "        \n",
    "# Paper figure\n",
    "    for ax in axs.flat:\n",
    "        ax.grid(False,)\n",
    "    # Add legend\n",
    "    labels = ['in-distribution', 'out-of-distribution']\n",
    "    handles = [\n",
    "        mpatches.Patch(facecolor='white', edgecolor='black', linewidth=0.5,label='in-distribution'),\n",
    "        mpatches.Patch(facecolor='white', edgecolor='black', linewidth=0.5,hatch='//////', label='out-of-distribution'),\n",
    "    ]\n",
    "    axs[0].legend(handles, labels, loc='best', ncol=1, fontsize=5,  frameon=False)\n",
    "    axs[0].set_ylabel(f'MAE')\n",
    "    axs[4].set_ylabel(f'MAE')\n",
    "    axs[-1].set_ylabel('Calibration Error', labelpad=0)\n",
    "    \n",
    "    \n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('../figures/Fig2_MAE_performance_comparison.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A. Calibration on CBPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(f'{RESULTS_FOLDER_NAME}', 'realized_metrics_dict.pkl'), 'rb') as f:\n",
    "    realized_metrics_dict = pkl.load(f)\n",
    "with open(os.path.join(f'{RESULTS_FOLDER_NAME}', 'CBPE_cali_metrics_dict.pkl'), 'rb') as f:\n",
    "    CBPE_metrics_dict = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE_dict = {'CBPE': {calib: {distr: {metric: [] for metric in METRICS} for distr in ['id_test', 'ood_test']} for calib in ['uncal', 'TS', 'CWTS', 'DE']},\n",
    "            }\n",
    "\n",
    "\n",
    "for model_name in MODEL_NAMES:\n",
    "    for seed in SEEDS:\n",
    "        if seed != 'seed_1':  # For now only look at seed 1\n",
    "            continue\n",
    "        for n, metric in enumerate(METRICS):\n",
    "            for calib in ['uncal', 'TS', 'CWTS']:\n",
    "                for m, distr in enumerate(['id_test', 'ood1_test', 'ood2_test']):\n",
    "                    ax = axs[m]\n",
    "                    realized = realized_metrics_dict[model_name][seed][distr][metric]\n",
    "                    CBPE = CBPE_metrics_dict[model_name][calib][seed][distr][metric]\n",
    "        \n",
    "                    # Calculate MAE for each metric\n",
    "                    if 'ood' in distr:\n",
    "                        distr = 'ood_test'\n",
    "    \n",
    "                    MAE_dict['CBPE'][calib][distr][metric].append(np.abs(realized - CBPE))          \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['hatch.linewidth'] = 0.5\n",
    "\n",
    "cmap_g = plt.get_cmap('Greens')\n",
    "\n",
    "color_mapping = {'test': 'k',\n",
    "                'validation': 'grey',\n",
    "                'CBPE': 'g',\n",
    "                'CBPE_TS': cmap_g(0.4),\n",
    "                'CBPE_CWTS': cmap_g(0.6)\n",
    "                }\n",
    "\n",
    "with plt.style.context('../config/plot_style.txt'): # type: ignore\n",
    "    fig, axs = plt.subplots(2, 4, figsize=(FIGURE_WIDTH, 0.5*FIGURE_WIDTH), layout='constrained', sharey=True, sharex=False) \n",
    "\n",
    "    axs = axs.flatten()\n",
    "    add_letters(fig, axs, dx=-1/72, dy=8/72) # Add letters to the subplots\n",
    "    for n, metric in enumerate(METRICS):\n",
    "\n",
    "        if metric == 'bal_accuracy':\n",
    "            axs[n].set_title('bal. accuracy')\n",
    "        elif metric == 'f1_score':\n",
    "            axs[n].set_title('F1-score')\n",
    "        elif metric == 'auc':\n",
    "            axs[n].set_title('AUC')\n",
    "        elif metric == 'precision':\n",
    "            axs[n].set_title('PPV')\n",
    "        else:\n",
    "            axs[n].set_title(metric)\n",
    "        axs[n].set_yticks([0, 0.2, 0.4, ])\n",
    "        for m, calib in enumerate(['uncal', 'TS', 'CWTS', 'DE']):   \n",
    "            for k, distr in enumerate(['id_test', 'ood_test']):\n",
    "                if calib == 'uncal':\n",
    "                    CBPE_mae = np.mean(MAE_dict['CBPE'][calib][distr][metric])\n",
    "                    CBPE_std = np.std(MAE_dict['CBPE'][calib][distr][metric])\n",
    "                                                        \n",
    "                    alpha = 1 if distr == 'ood_test' else 1\n",
    "                    hatch = '//////' if distr == 'ood_test' else ''\n",
    "                    axs[n].bar(k*0.2, CBPE_mae, width=0.2, linewidth=0.5,label='CBPE', edgecolor='black', color=color_mapping['CBPE'], alpha=alpha, hatch=hatch,)# yerr=CBPE_std, capsize=1)\n",
    "                \n",
    "                elif calib=='TS':\n",
    "                    CBPE_mae = np.mean(MAE_dict['CBPE'][calib][distr][metric])\n",
    "                    CBPE_std = np.std(MAE_dict['CBPE'][calib][distr][metric]) \n",
    "                    alpha = 1 if distr == 'ood_test' else 1\n",
    "                    hatch = '//////' if distr == 'ood_test' else ''\n",
    "                    axs[n].bar(k*0.2 + 0.5, CBPE_mae,linewidth=0.5, width=0.2, label='CBPE', color=color_mapping['CBPE_TS'], edgecolor='black', alpha=alpha, hatch=hatch,)# yerr=CBPE_std, capsize=1)\n",
    "        \n",
    "                elif calib == 'CWTS': # Add CWTS for CBPE\n",
    "                    CBPE_mae = np.mean(MAE_dict['CBPE'][calib][distr][metric])\n",
    "                    CBPE_std = np.std(MAE_dict['CBPE'][calib][distr][metric]) \n",
    "                    alpha = 1 if distr == 'ood_test' else 1\n",
    "                    hatch = '//////' if distr == 'ood_test' else ''\n",
    "                    axs[n].bar(k*0.2 + 1, CBPE_mae,linewidth=0.5, width=0.2, label='CBPE', color=color_mapping['CBPE_CWTS'], edgecolor='black', alpha=alpha, hatch=hatch,)# yerr=CBPE_std, capsize=1)\n",
    "                    \n",
    "                if n >=4:\n",
    "                    axs[n].set_xticks([0.1, 0.6, 1.1])\n",
    "                    axs[n].set_xticklabels(['CBPE', 'CBPE\\nts','CBPE\\ncsts'], rotation=0, ha='center',)\n",
    "                else:\n",
    "                    #axs[n].set_xticks([0.1, 0.5, 1.0, 1.5, 2.0, 2.5, 3.])\n",
    "                    axs[n].set_xticks([0.1, 0.6, 1.1,])\n",
    "                    axs[n].set_xticklabels([])\n",
    "\n",
    "                    \n",
    "    ax = axs[-1]\n",
    "    for l, calib in enumerate(['', 'TS_', 'CWTS_', 'DE_']):\n",
    "        if calib == 'DE_':\n",
    "            continue  \n",
    "        ax.set_title('Calibration')\n",
    "        \n",
    "        rbs = np.mean([realized_metrics_dict[model_name]['seed_1']['id_test'][f'{calib}rbs'] for model_name in MODEL_NAMES])\n",
    "        ace = np.mean([realized_metrics_dict[model_name]['seed_1']['id_test'][f'{calib}ACE'] for model_name in MODEL_NAMES])\n",
    "        ax.bar(0+l, rbs, width=0.2, linewidth=0.5,label='RBS', color='grey', edgecolor='black',alpha=1,)\n",
    "        ax.bar(0.5+l, ace, width=0.2, linewidth=0.5,label='ACE', color='orange', edgecolor='black', alpha=1,)       \n",
    "        rbs = np.mean([[realized_metrics_dict[model_name]['seed_1'][distr][f'{calib}rbs'] for distr in ['ood1_test', 'ood2_test'] ] for model_name in MODEL_NAMES])\n",
    "        ace = np.mean([[realized_metrics_dict[model_name]['seed_1'][distr][f'{calib}ACE'] for distr in ['ood1_test', 'ood2_test'] ] for model_name in MODEL_NAMES])\n",
    "        ax.bar(0.2+l, rbs, width=0.2,linewidth=0.5, color='grey', edgecolor='black', hatch='///', )\n",
    "        ax.bar(0.7+l, ace, width=0.2,linewidth=0.5, color='orange', edgecolor='black', hatch='///', )\n",
    "        \n",
    "        ax.set_xticks([0.1, 0.6, 1.1, 1.6, 2.1, 2.6,])\n",
    "        ax.set_xticklabels(['RBS', 'ACE', 'RBS\\nts', 'ACE\\nts', 'RBS\\ncsts', 'ACE\\ncsts'], rotation=0, ha='center')\n",
    "        \n",
    "        \n",
    "\n",
    "    for ax in axs.flat:\n",
    "        ax.grid(False,)\n",
    "    # Add legend\n",
    "    labels = ['in-distribution', 'out-of-distribution']\n",
    "    handles = [\n",
    "        mpatches.Patch(facecolor='white', edgecolor='black', linewidth=0.5,label='in-distribution'),\n",
    "        mpatches.Patch(facecolor='white', edgecolor='black', linewidth=0.5,hatch='//////', label='out-of-distribution'),\n",
    "    ]\n",
    "    axs[0].legend(handles, labels, loc='best', ncol=1, fontsize=5,  frameon=False)\n",
    "    axs[0].set_ylabel(f'MAE')\n",
    "    axs[4].set_ylabel(f'MAE')\n",
    "    axs[-1].set_ylabel('Calibration Error', labelpad=0)\n",
    "    \n",
    "    \n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('../figures/A2_CBPE_calibration.pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "performance_metrics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
